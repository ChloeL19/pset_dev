{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the goal in this notebook is to use transfer learning on the CIFAR dataset\n",
    "\n",
    "# import libraries necessary for building the CNN\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Conv2D, Dense, Activation, Flatten, MaxPooling2D, Dropout, ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for plotting the model\n",
    "# !pip install pydot\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important variables\n",
    "num_classes = 10 # one for each of the digits from 0-9\n",
    "batch_size = 128 #tunable number\n",
    "epochs = 13 #tunable number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    }
   ],
   "source": [
    "# import cifar10 data from keras\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# I just want to see what these images look like\n",
    "print(X_train[0].shape)\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# reshape the x data do it's 4 dimensional --> last number is color channels\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "\n",
    "\n",
    "# we need to normalize the X data before feeding into our model\n",
    "X_train = X_train/255 # divide by max value of pixel values to put in range 0 to 1\n",
    "X_test = X_test/255\n",
    "\n",
    "# we also need to convert the Y data into one-hot vectors\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes) # may want to explain this more clearly [background]\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "# for interpretation purposes, create a dictionary mapping the label numbers\n",
    "# to image labels\n",
    "num_to_img = {\n",
    "    0:\"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}\n",
    "\n",
    "print(num_to_img[np.argmax(Y_train[0])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 48, 48, 256)\n"
     ]
    }
   ],
   "source": [
    "# pad the normalized input images in order for transfer learning to work\n",
    "input_shape = [32, 32, 3]\n",
    "X_input = Input(input_shape)\n",
    "X = ZeroPadding2D(padding=(8,8))(X_input)\n",
    "\n",
    "# loading the VGG16 model for transfer learning\n",
    "my_vgg = vgg16.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "# freezing the transfered layers\n",
    "#for layer in my_vgg.layers:\n",
    "    #print(layer.name)\n",
    "    \n",
    "# transition conv layer\n",
    "X = Conv2D(128, kernel_size=(1,1))(X)\n",
    "X = my_vgg.get_layer(\"block3_conv1\")(X)\n",
    "#X = my_vgg.get_layer(\"block3_conv2\")(X)\n",
    "#X = my_vgg.get_layer(\"block3_pool\")(X)\n",
    "print(X.shape)\n",
    "#X = Conv2D(16, kernel_size=(1,1))(X)\n",
    "#X = Activation('relu')(X)\n",
    "\n",
    "X = Flatten()(X)\n",
    "X = Dense(num_classes, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, #\n",
    "             optimizer=keras.optimizers.Adam(), #students can choose optimizers \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 48, 48, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 48, 48, 128)       512       \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        multiple                  295168    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 589824)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5898250   \n",
      "=================================================================\n",
      "Total params: 6,193,930\n",
      "Trainable params: 6,193,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/13\n",
      "  512/50000 [..............................] - ETA: 1:11:17 - loss: 9.8096 - acc: 0.1172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1e830e9c95ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chloeloughridge/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model on the training data\n",
    "model.fit(X_train, Y_train, batch_size = batch_size, epochs=epochs, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 158.92 337.00\" width=\"159pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 154.9209,-333 154.9209,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 47694779336 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>47694779336</title>\n",
       "<polygon fill=\"none\" points=\"11.2793,-292.5 11.2793,-328.5 139.6416,-328.5 139.6416,-292.5 11.2793,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.4604\" y=\"-306.3\">input_4: InputLayer</text>\n",
       "</g>\n",
       "<!-- 47713237368 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>47713237368</title>\n",
       "<polygon fill=\"none\" points=\"12.4346,-219.5 12.4346,-255.5 138.4863,-255.5 138.4863,-219.5 12.4346,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.4604\" y=\"-233.3\">conv2d_4: Conv2D</text>\n",
       "</g>\n",
       "<!-- 47694779336&#45;&gt;47713237368 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>47694779336-&gt;47713237368</title>\n",
       "<path d=\"M75.4604,-292.4551C75.4604,-284.3828 75.4604,-274.6764 75.4604,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"78.9605,-265.5903 75.4604,-255.5904 71.9605,-265.5904 78.9605,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 47713236080 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>47713236080</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 150.9209,-182.5 150.9209,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.4604\" y=\"-160.3\">activation_3: Activation</text>\n",
       "</g>\n",
       "<!-- 47713237368&#45;&gt;47713236080 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>47713237368-&gt;47713236080</title>\n",
       "<path d=\"M75.4604,-219.4551C75.4604,-211.3828 75.4604,-201.6764 75.4604,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"78.9605,-192.5903 75.4604,-182.5904 71.9605,-192.5904 78.9605,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 47713236360 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>47713236360</title>\n",
       "<polygon fill=\"none\" points=\"19.8276,-73.5 19.8276,-109.5 131.0933,-109.5 131.0933,-73.5 19.8276,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.4604\" y=\"-87.3\">flatten_3: Flatten</text>\n",
       "</g>\n",
       "<!-- 47713236080&#45;&gt;47713236360 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>47713236080-&gt;47713236360</title>\n",
       "<path d=\"M75.4604,-146.4551C75.4604,-138.3828 75.4604,-128.6764 75.4604,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"78.9605,-119.5903 75.4604,-109.5904 71.9605,-119.5904 78.9605,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 47702093216 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>47702093216</title>\n",
       "<polygon fill=\"none\" points=\"23.3345,-.5 23.3345,-36.5 127.5864,-36.5 127.5864,-.5 23.3345,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.4604\" y=\"-14.3\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 47713236360&#45;&gt;47702093216 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>47713236360-&gt;47702093216</title>\n",
       "<path d=\"M75.4604,-73.4551C75.4604,-65.3828 75.4604,-55.6764 75.4604,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"78.9605,-46.5903 75.4604,-36.5904 71.9605,-46.5904 78.9605,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing model architecture\n",
    "#import pydot\n",
    "#plot_model(model, to_file='model01.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format=\"svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# going to visualize the filters of the model  . . . \n",
    "# inspiration from here: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "\n",
    "# create a dictionary of the model's layers\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_8/conv2d_4/convolution_grad/Conv2DBackpropInput:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K #the backend in this case is tensorflow, since keras is sitting on top of it\n",
    "                                #so importing the \"backend\" will allow us to write code in tensorflow that keras\n",
    "                                #will be able to use for our model\n",
    "        \n",
    "#we'll be sending in an image the size of one of the MNIST images\n",
    "img_width = 32\n",
    "img_height = 32\n",
    "        \n",
    "layer_name = \"conv2d_4\"\n",
    "filter_index = 3 #there are 32 filters, so this could be any number in the range 0-31\n",
    "\n",
    "# create a loss function that we will use to maximize the activation of the specified layer\n",
    "layer_output = layer_dict[layer_name].output # accesing the output of the specified layer stored in our dictionary\n",
    "loss = K.mean(layer_output[:,:,:,filter_index]) # averaging all the outputs of the filter --> remember that the filter\n",
    "                                                # is a 2-D \"square\" --> the middle two numbers represent height/width\n",
    "                                                # of that square, the first number represents batch size, and the \n",
    "                                                # final number represents the number of filters (we're accessing a\n",
    "                                                # specific one here)\n",
    "# compute the gradient of the input picture with respect to this loss.\n",
    "# this means we'll be updating the pixels of the input image, not the weights of the network --> clever!\n",
    "grads = K.gradients(loss, model.input)[0] # I don't know what the [0] means at the end of this line\n",
    "print(grads)\n",
    "\n",
    "# normalizing the gradient --> we don't want the magnitude of our gradient ascent/descent step to be infuenced heavily \n",
    "# by the gradient --> the gradient gives us the direction we want to take --> so normalizing helps the optimization\n",
    "# algorithm perform better\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "iterate = K.function([model.input], [loss, grads]) # this is a fancy way of writing a custom function that takes\n",
    "                                                # an input image as an input, and returns the loss and gradients\n",
    "                                                # for that image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# running gradient ascent to maximize the activations of the filter\n",
    "\n",
    "# generating a grey test image\n",
    "input_img = np.random.random((1, img_width, img_height, 3)) * 20 + 128.\n",
    "\n",
    "# Choosing an arbitrary value for \"step\" --> pretty sure this is like the learning rate\n",
    "step = 5\n",
    "\n",
    "# 20 steps of gradient ascent\n",
    "for i in range(50):\n",
    "    loss_value, grads_value = iterate([input_img])\n",
    "    input_img += grads_value * step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a function to deprocess the image \n",
    "from scipy.misc import imsave\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to black/white representation\n",
    "    x *= 255\n",
    "    #x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1PxCJIqaX4hEUfMLkSj/B/fbXTLC66zDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the image as a file\n",
    "img = input_img\n",
    "img = deprocess_image(img)\n",
    "\n",
    "print(img.squeeze().shape)\n",
    "\n",
    "# the imsave approach\n",
    "#imsave('%s_filter_%d.png' % (layer_name, filter_index), img)\n",
    "\n",
    "# if color channels are first\n",
    "#img_print = np.swapaxes(img.squeeze(), 0, 2)\n",
    "\n",
    "# the matplotlib approach\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# upload the custom image and have the network make a prediction on it\n",
    "\n",
    "# require students to take square images --> for scaling purposes\n",
    "\n",
    "import cv2\n",
    "\n",
    "# load the image in greyscale mode\n",
    "my_img = cv2.imread(\"./my_img.jpg\", 0)\n",
    "plt.imshow(my_img)\n",
    "plt.show()\n",
    "\n",
    "# reshape the custom image\n",
    "my_img = cv2.resize(my_img, (32,32,3))\n",
    "plt.imshow(my_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
